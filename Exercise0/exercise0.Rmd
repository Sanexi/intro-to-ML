---
title: "Exercise set 0"
output: pdf_document
---
```{r setup, include=FALSE}
library("reticulate")
rmarkdown::render
#knitr::knit_engines$set(python = reticulate::eng_python)
#tinytex::install_tinytex()
```


## Problem 1

### Task a

Load the data set in x.csv:
```{python}
import pandas as pd

x = pd.read_csv("x.csv")
x
```

Finding the two variables having the largest variances:
```{python}
import matplotlib.pyplot as plt

largest = x.var().idxmax()
largest_df = x.pop(largest).to_frame()

large = x.var().idxmax()
large_df = x.pop(large).to_frame()

new_x = large_df.join(largest_df)
```

Plotting variables:
```{python}
new_x.plot(x=large, y=largest, kind='scatter')
plt.show()
```


## Problem 2

### Task a

Let's break down B:
$$B = \sum_{j=1}^n \lambda_jx_jx_j^T =
\begin{pmatrix} x_1 & x_2 & ... & x_n \end{pmatrix} 
\begin{pmatrix} \lambda_1 & 0 & ... & 0 \\ 0 & \lambda_2 & ... & 0 \\ ... \\ 0 & 0 & ... &  \lambda_n \end{pmatrix}
\begin{pmatrix} x_1^T \\  x_2^T \\ ... \\  x_n^T \end{pmatrix}$$

For $\lambda_1$ and $x_1$ to be the eigenvalues/-vectors of matrix B we must prove that $$B_i x_i=\lambda_i x_i\ \forall i=1..n$$

Now we have the following:
$$\ B_ix_i=\lambda_i\begin{pmatrix}
x_{1i}x_{1i} & ... & x_{1i}x_{ni} \\ 
 ... & ... & ... \\ 
x_{ni}x_{1i} & ... & ... x_{ni}x_{ni}
\end{pmatrix} \begin{pmatrix}
x_{1i}\\ 
... \\ 
x_{ni}
\end{pmatrix}
= \lambda_i\begin{pmatrix}
x_{1i}\sum_{j=1}^{n}x_{ji}^2\\ 
...\\ 
x_{ni}\sum_{j=1}^{n}x_{ji}^2
\end{pmatrix}$$

The last vector is orthonormal giving it the sum of 1 and proving that the vector is $x_i$. This proves that $$B_ix_i=\lambda_ix_i\ \forall i=1,...,n$$
And so $\lambda_i$ and $x_i$ are eigenvalues/-vectors of B.

### Task b

$$\begin{vmatrix} 1 & 2 \\ 2 & 3.14159 \end{vmatrix}
\rightarrow
\begin{vmatrix} -\lambda+1 & 2 \\ 2 & -\lambda+3.14159 \end{vmatrix}$$
With calculator:
Eigenvalues:
$\lambda_1 = \frac{\sqrt {205864077281} + 414159} {200000}$
$\lambda_2 = -\frac{\sqrt {205864077281} + 414159} {200000}$
Eigenvectors:
$x_1 = \frac{\sqrt {205864077281} + 214159} {400000}*x_2$
$x_2 = x_1$


## Problem 3

### Task a

In order for $E$ to be a linear operator, we need to show that: $E[X+Y]=E[X]+E[Y]$ and $E[tX]=tE[X]$.

Now $$E[X+Y]=\sum_{\omega\in\Omega}(X+Y)(\omega)P(\omega)
= \sum_{\omega\in\Omega}X(\omega)P(\omega)+\sum_{\omega\in\Omega}Y(\omega)P(\omega)$$
This gives us: $$E[X+Y] = E[X]+E[Y]$$

$$E[tX]=\sum_{\omega\in\Omega}tX(\omega)P(\omega)=t\sum_{\omega\in\Omega}X(\omega)P(\omega)$$
Which gives us: $$E[tX]=tE[X]$$

According to this proof we can declare that $E$ is a linear operator.

### Task b
$\operatorname{Var}(X) = E[(X-\mu)^2] = E[x^2-2\mu x+\mu^2] = E[x^2]-E[\mu(2x-\mu)] = E[x^2]-\mu E[2x-\mu] = E[x^2]-u*(2u-u) = E[X^2]-E[X]^2$


## Problem 4

### Task a
$$P(X \cap Y) = P(Y|X) P(X)$$
Formula for conditional probability:
$$P(X|Y) = \frac{P(X \cap Y)}{P(Y)} = \frac{P(Y|X) P(X)}{P(Y)}$$




### Task b

Allergic:

| Y   | N   |
|-----|-----|
| 0.2 | 0.8 |

Test result:

| Allergic | Pos  | Neg  |
|----------|------|------|
| 1        | 0.85 | 0.15 |
| 0        | 0.23 | 0.77 |

Calculating ratio:
$$R = \frac{P("Allergic"|"Pos")}{P("Not allergic"|"Pos")}
= \frac{P("Pos"|"Allergic") P("Allergic")}
{P("Pos"|"Not allergic") P("Not allergic")}
= \frac{0.85*0.2}{0.23*0.8} = 0.17/0.184 = 0.92391...$$
$$P("Allergic"|"Pos")
= \frac{R}{1+R}
= 0.4802259...$$
Probability is about 48%


## Problem 5

### Task a
$$f'(x) = 4ax^3+b = 0 $$
$$f''(x_o) = 0, x_0 = \frac{-b}{4a} $$ (Minimum)
Following:
$$f_{min}=f(x_o) =\frac{3b}{4}(\frac{-b}{4a})^{1/3}+c $$

### Task b
Only condition is $a > 0$.


## Problem 6

### Task a
```
function fibonacci(n):
  counter = 0
  fib1 = 0
  fib2 = 1
  while counter < n:
    counter = counter+1
    print(fib1)
    sum = fib1 + fib2
    fib1 = n2
    fib2 = sum
```

### Task b
O(n)