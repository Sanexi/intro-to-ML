---
title: "Exercise Set 2"
output: pdf_document
---
```{r setup, include=FALSE}
library("reticulate")
rmarkdown::render
knitr::knit_engines$set(python = reticulate::eng_python)
#tinytex::install_tinytex()
```

## Problem 8

### Task a

```{python, eval=FALSE}
import math
import pandas as pd
from sklearn import linear_model
from sklearn.metrics import accuracy_score

spam_train = pd.read_csv("spam_train.csv")
spam_test = pd.read_csv("spam_test.csv")

clf = linear_model.LogisticRegression(penalty="l1", C=1/0.001, solver="saga", max_iter=2000)

X_train = spam_train.iloc[:,:5]
y_train = spam_train.iloc[:,5]
X_test = spam_test.iloc[:,:5]
y_test = spam_test.iloc[:,5]

clf.fit(X_train, y_train)

print("Intercept", clf.intercept_[0])
print("Coefficients:")
for c, v in zip(spam_test.columns[:5], clf.coef_[0]):
  print(c, v)
  
pred_train = clf.predict(X_train)
train_acc = accuracy_score(y_train, pred_train)
pred_test = clf.predict(X_test)
test_acc = accuracy_score(y_test, pred_test)

print("Accuracy train:", train_acc)
print("Accuracy test:", test_acc)

phat_train = clf.predict_proba(X_train)[:,1]
phat_test = clf.predict_proba(X_test)[:,1]

per_train = 0
for i in phat_train:
  per_train += math.log(i)
print("Perplexity train:", -per_train)

per_test = 0
for i in phat_test:
  per_test += math.log(i)
print("Perplexity test:", -per_test)
```

Convergence warning is due to hitting the iteration limit on the logistic regression model. By increasing iterations limit we can solve this warning. Plot:
![](plot1.png)


### Task b

```{python, eval=FALSE}
lasso = linear_model.Lasso(alpha=0.1)

lasso.fit(X_train, y_train)

for c, v in zip(spam_test.columns[:5], lasso.coef_):
  print(c, v)

pred_train = lasso.predict(X_train)
pred_test = lasso.predict(X_test)

lassoper_train = 0
for i in pred_train:
  lassoper_train += math.log(i)
print("Perplexity train:", -lassoper_train)

lassoper_test = 0
for i in pred_test:
  lassoper_test += math.log(i)
print("Perplexity test:", -lassoper_test)
```

alpha=0.1, for example, gives all but T_FILL_THIS_FORM_SHORT coef to 0/-0.


## Problem 9

### Task a


## Problem 11

### Task a

The authors say that the difference between discriminative and generative learning model isn't always clear even though many seem to think that discriminative is obviously better. The larger the dataset the larger asymptotic error the generative model has, over the discriminative model, but if the generative model has already reached its asymptotic error it usually performs better.

### Task b

$h_{Gen}$: a generative model chosen by optimizing the joint likelihood of the inputs and the labels.
$h_{Dis}$: a discriminative model chosen or by optimizing the conditional likelihood or minimizing the 0-1 training error.

### Task c

In these graphs the x-axis is "m"; random samples from train/test splits, and y-axis which is the error of the Naive Bayes and Logistic Regression models. Results of these graphs are very mixed. Notable ones are `liver disorders` and `lenses`, where Logistic Regression starts performing better from a certain m, which is what we discussed in Task a.


