perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LinearRegression(fit_intercept=False)
m_LRx = LinearRegression()
m_NBx = GaussianNB(class_prior=[])
#m_D
#m_kNN
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
m_NB.class_prior_
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB(class_prior=[0.44433594, 0.55566406])
m_LR = LinearRegression(fit_intercept=False)
m_LRx = LinearRegression()
m_NBx = GaussianNB(class_prior=[])
#m_D
#m_kNN
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
m_NB.class_prior_
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LinearRegression(fit_intercept=False)
m_LRx = LinearRegression()
m_NBx = GaussianNB(class_prior=0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no fit"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no fit"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR fit"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR fit"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(class_prior=0.444, 0.566])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no inter"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no inter"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(class_prior=[0.444, 0.566])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no inter"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no inter"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(class_prior_=[0.444, 0.566])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no inter"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no inter"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(priors=[0.444, 0.566])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no inter"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no inter"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR no inter"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR no inter"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB with prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB with prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="elasticnet")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none")
m_LRx = LogisticRegression(penalty="l2")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none", solver="saga")
m_LRx = LogisticRegression(penalty="elasticnet", solver="saga")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none", solver="newton-cg")
m_LRx = LogisticRegression(penalty="elasticnet", solver="newton-cg")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
ns = [2**i for i in range(3,13)]
data = [pd.read_csv(f'toy_train_{n}.csv') for n in ns]
data_test = pd.read_csv("toy_test.csv")
y_test = data_test["y"]
phat = lambda m: [
m.fit(d.iloc[:, :2], d["y"]).predict_proba(data_test.iloc[:, :2])[:, 1]
for d in data
]
accuracy = lambda p: (y_test * np.round(p) + (1 - y_test) * (1 - np.round(p))).mean()
perplexity = lambda p: np.exp(-np.mean(np.log(y_test*p + (1 - y_test) * (1 - p))))
m_NB = GaussianNB()
m_LR = LogisticRegression(penalty="none", solver="newton-cg")
m_LRx = LogisticRegression(penalty="l2", solver="newton-cg")
m_NBx = GaussianNB(priors=[0.4, 0.6])
m_D = DummyClassifier()
m_kNN = KNeighborsClassifier()
# Store results in a pandas dataframe
res_acc = pd.DataFrame(index=ns)
res_perp = pd.DataFrame(index=ns)
res_acc["NB"] = [accuracy(p) for p in phat(m_NB)]
res_perp["NB"] = [perplexity(p) for p in phat(m_NB)]
res_acc["LR"] = [accuracy(p) for p in phat(m_LR)]
res_perp["LR"] = [perplexity(p) for p in phat(m_LR)]
res_acc["LR inter"] = [accuracy(p) for p in phat(m_LRx)]
res_perp["LR inter"] = [perplexity(p) for p in phat(m_LRx)]
res_acc["NB prior"] = [accuracy(p) for p in phat(m_NBx)]
res_perp["NB prior"] = [perplexity(p) for p in phat(m_NBx)]
res_acc["Dummy"] = [accuracy(p) for p in phat(m_D)]
res_perp["Dummy"] = [perplexity(p) for p in phat(m_D)]
res_acc["kNN"] = [accuracy(p) for p in phat(m_kNN)]
res_perp["kNN"] = [perplexity(p) for p in phat(m_kNN)]
res_acc
res_perp
setwd("~/Coding/intro-to-ML/Exercise2")
